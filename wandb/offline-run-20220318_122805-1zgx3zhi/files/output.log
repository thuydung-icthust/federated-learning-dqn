>>> START RUNNING: FedAVG-Equal-Non-IID-01-2.1.1 - Train mode: benchmark - Dataset: chexpert
START LOADING CHEXPERT DATASET...
Init State dim 6
Init Action dim 3
ROUND:  0
[5, 5, 5]
  0%|                                        | 0/1000 [00:10<?, ?it/s]
multiprocessing.pool.RemoteTraceback:
"""
Traceback (most recent call last):
  File "/home/aiotlab/anaconda3/envs/thanhnt_vaipe/lib/python3.9/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/home/aiotlab/anaconda3/envs/thanhnt_vaipe/lib/python3.9/multiprocessing/pool.py", line 48, in mapstar
    return list(map(*args))
  File "/mnt/disk1/dungnt/federated-learning-dqn/utils/trainer.py", line 40, in train
    _, start_inference_loss = test(model, train_dataloader)
  File "/mnt/disk1/dungnt/federated-learning-dqn/utils/trainer.py", line 90, in test
    output = model(X)
  File "/home/aiotlab/anaconda3/envs/thanhnt_vaipe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/disk1/dungnt/federated-learning-dqn/models/resnet18.py", line 122, in forward
    output = self.conv1(x)
  File "/home/aiotlab/anaconda3/envs/thanhnt_vaipe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/aiotlab/anaconda3/envs/thanhnt_vaipe/lib/python3.9/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/aiotlab/anaconda3/envs/thanhnt_vaipe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/aiotlab/anaconda3/envs/thanhnt_vaipe/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py", line 168, in forward
    return F.batch_norm(
  File "/home/aiotlab/anaconda3/envs/thanhnt_vaipe/lib/python3.9/site-packages/torch/nn/functional.py", line 2282, in batch_norm
    return torch.batch_norm(
RuntimeError: CUDA out of memory. Tried to allocate 124.00 MiB (GPU 0; 9.78 GiB total capacity; 170.90 MiB already allocated; 83.88 MiB free; 188.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
"""
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File "/mnt/disk1/dungnt/federated-learning-dqn/train.py", line 344, in <module>
    main(args)
  File "/mnt/disk1/dungnt/federated-learning-dqn/train.py", line 199, in main
    pool.map(
  File "/home/aiotlab/anaconda3/envs/thanhnt_vaipe/lib/python3.9/multiprocessing/pool.py", line 364, in map
    return self._map_async(func, iterable, mapstar, chunksize).get()
  File "/home/aiotlab/anaconda3/envs/thanhnt_vaipe/lib/python3.9/multiprocessing/pool.py", line 771, in get
    raise self._value
RuntimeError: CUDA out of memory. Tried to allocate 124.00 MiB (GPU 0; 9.78 GiB total capacity; 170.90 MiB already allocated; 83.88 MiB free; 188.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF