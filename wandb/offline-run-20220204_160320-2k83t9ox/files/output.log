>>> START RUNNING: FedRL-Fixed-Quantitative-Non-IID-01-dung-lstm - Train mode: RL-Fixed - Dataset: fashionmnist
Init State dim 60
Init Action dim 10
ROUND:  0
[5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
  0%|                                                  | 0/1000 [00:09<?, ?it/s]
multiprocessing.pool.RemoteTraceback:
"""
Traceback (most recent call last):
  File "/home/aimenext/anaconda3/envs/dungnt-fl/lib/python3.9/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/home/aimenext/anaconda3/envs/dungnt-fl/lib/python3.9/multiprocessing/pool.py", line 48, in mapstar
    return list(map(*args))
  File "/home/aimenext/dungnt/federated-learning-dqn/utils/trainer.py", line 35, in train
    model = model.to(device)
  File "/home/aimenext/anaconda3/envs/dungnt-fl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/aimenext/anaconda3/envs/dungnt-fl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/aimenext/anaconda3/envs/dungnt-fl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/aimenext/anaconda3/envs/dungnt-fl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
"""
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File "/home/aimenext/dungnt/federated-learning-dqn/train.py", line 326, in <module>
    main(args)
  File "/home/aimenext/dungnt/federated-learning-dqn/train.py", line 183, in main
    pool.map(
  File "/home/aimenext/anaconda3/envs/dungnt-fl/lib/python3.9/multiprocessing/pool.py", line 364, in map
    return self._map_async(func, iterable, mapstar, chunksize).get()
  File "/home/aimenext/anaconda3/envs/dungnt-fl/lib/python3.9/multiprocessing/pool.py", line 771, in get
    raise self._value
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.