>>> START RUNNING: FedAVG-Equal-Non-IID-01-2.1.1 - Train mode: benchmark - Dataset: chexpert
START LOADING CHEXPERT DATASET...
Init State dim 6
Init Action dim 3
ROUND:  0
[5, 5, 5]
output: tensor([[ 0.1781, -0.1543,  1.1753,  ...,  0.4073,  1.1384,  0.6438],
        [ 0.0033, -0.2467,  1.2580,  ...,  0.3981,  1.3147,  0.6067],
        [ 0.1553, -0.0741,  0.5195,  ...,  0.2830,  0.8680,  0.3624],
        ...,
        [ 0.2258, -0.1416,  0.1477,  ...,  0.2450,  0.5825,  0.3328],
        [ 0.1705, -0.0952,  0.1331,  ...,  0.0444,  0.7946,  0.5136],
        [ 0.2344, -0.1279,  0.3854,  ...,  0.1451,  0.7863,  0.3148]],
       device='cuda:0', grad_fn=<AddmmBackward0>)
x: tensor([[[[-1.5528, -1.5528, -1.5528,  ..., -1.3987, -1.3815, -1.2959],
          [-1.6213, -1.6213, -1.6042,  ..., -1.3815, -1.3644, -1.3302],
          [-1.7754, -1.7754, -1.7583,  ..., -1.3987, -1.3815, -1.2959],
          ...,
          [-1.9638, -1.9467, -1.9124,  ..., -1.2959, -1.2617, -1.2617],
          [-1.9295, -1.9467, -1.9295,  ..., -1.2959, -1.2617, -1.2617],
          [-1.9467, -1.9467, -1.9295,  ..., -1.3130, -1.2788, -1.2617]],
         [[-1.4580, -1.4580, -1.4580,  ..., -1.3004, -1.2829, -1.1954],
          [-1.5280, -1.5280, -1.5105,  ..., -1.2829, -1.2654, -1.2304],
          [-1.6856, -1.6856, -1.6681,  ..., -1.3004, -1.2829, -1.1954],
          ...,
          [-1.8782, -1.8606, -1.8256,  ..., -1.1954, -1.1604, -1.1604],
          [-1.8431, -1.8606, -1.8431,  ..., -1.1954, -1.1604, -1.1604],
          [-1.8606, -1.8606, -1.8431,  ..., -1.2129, -1.1779, -1.1604]],
         [[-1.2293, -1.2293, -1.2293,  ..., -1.0724, -1.0550, -0.9678],
          [-1.2990, -1.2990, -1.2816,  ..., -1.0550, -1.0376, -1.0027],
          [-1.4559, -1.4559, -1.4384,  ..., -1.0724, -1.0550, -0.9678],
          ...,
          [-1.6476, -1.6302, -1.5953,  ..., -0.9678, -0.9330, -0.9330],
          [-1.6127, -1.6302, -1.6127,  ..., -0.9678, -0.9330, -0.9330],
          [-1.6302, -1.6302, -1.6127,  ..., -0.9853, -0.9504, -0.9330]]],
        [[[-1.0219, -1.3815, -1.6213,  ..., -1.8097, -1.8097, -1.8097],
          [-1.0390, -1.3644, -1.6042,  ..., -1.8439, -1.8439, -1.8439],
          [-1.0562, -1.3644, -1.6042,  ..., -1.8953, -1.8953, -1.8953],
          ...,
          [-0.8164, -1.2445, -1.5357,  ..., -1.9124, -1.8953, -1.8782],
          [-0.8164, -1.2445, -1.5185,  ..., -1.9124, -1.9124, -1.8953],
          [-0.7650, -1.2274, -1.5357,  ..., -1.8782, -1.8782, -1.8782]],
         [[-0.9153, -1.2829, -1.5280,  ..., -1.7206, -1.7206, -1.7206],
          [-0.9328, -1.2654, -1.5105,  ..., -1.7556, -1.7556, -1.7556],
          [-0.9503, -1.2654, -1.5105,  ..., -1.8081, -1.8081, -1.8081],
          ...,
          [-0.7052, -1.1429, -1.4405,  ..., -1.8256, -1.8081, -1.7906],
          [-0.7052, -1.1429, -1.4230,  ..., -1.8256, -1.8256, -1.8081],
          [-0.6527, -1.1253, -1.4405,  ..., -1.7906, -1.7906, -1.7906]],
         [[-0.6890, -1.0550, -1.2990,  ..., -1.4907, -1.4907, -1.4907],
          [-0.7064, -1.0376, -1.2816,  ..., -1.5256, -1.5256, -1.5256],
          [-0.7238, -1.0376, -1.2816,  ..., -1.5779, -1.5779, -1.5779],
          ...,
          [-0.4798, -0.9156, -1.2119,  ..., -1.5953, -1.5779, -1.5604],
          [-0.4798, -0.9156, -1.1944,  ..., -1.5953, -1.5953, -1.5779],
          [-0.4275, -0.8981, -1.2119,  ..., -1.5604, -1.5604, -1.5604]]],
        [[[-1.2959, -1.2959, -1.2788,  ..., -0.1999, -0.2684, -0.3027],
          [-1.2788, -1.2617, -1.1760,  ..., -0.1999, -0.2856, -0.3198],
          [-1.2445, -1.1418, -0.9363,  ..., -0.2171, -0.3198, -0.3712],
          ...,
          [-0.0972, -0.1143, -0.1314,  ..., -1.4329, -1.5014, -1.5528],
          [ 0.0056, -0.0801, -0.2342,  ..., -1.3644, -1.5185, -1.5870],
          [ 0.0398, -0.0629, -0.2684,  ..., -1.3302, -1.5185, -1.6042]],
         [[-1.1954, -1.1954, -1.1779,  ..., -0.0749, -0.1450, -0.1800],
          [-1.1779, -1.1604, -1.0728,  ..., -0.0749, -0.1625, -0.1975],
          [-1.1429, -1.0378, -0.8277,  ..., -0.0924, -0.1975, -0.2500],
          ...,
          [ 0.0301,  0.0126, -0.0049,  ..., -1.3354, -1.4055, -1.4580],
          [ 0.1352,  0.0476, -0.1099,  ..., -1.2654, -1.4230, -1.4930],
          [ 0.1702,  0.0651, -0.1450,  ..., -1.2304, -1.4230, -1.5105]],
         [[-0.9678, -0.9678, -0.9504,  ...,  0.1476,  0.0779,  0.0431],
          [-0.9504, -0.9330, -0.8458,  ...,  0.1476,  0.0605,  0.0256],
          [-0.9156, -0.8110, -0.6018,  ...,  0.1302,  0.0256, -0.0267],
          ...,
          [ 0.2522,  0.2348,  0.2173,  ..., -1.1073, -1.1770, -1.2293],
          [ 0.3568,  0.2696,  0.1128,  ..., -1.0376, -1.1944, -1.2641],
          [ 0.3916,  0.2871,  0.0779,  ..., -1.0027, -1.1944, -1.2816]]],
        ...,
        [[[ 0.3481,  0.5193,  0.8104,  ..., -0.4911, -0.4568, -0.4397],
          [ 0.5193,  0.5536,  0.5878,  ..., -0.7650, -0.7137, -0.6965],
          [ 0.7933,  0.6049,  0.2624,  ..., -1.1932, -1.1247, -1.0733],
          ...,
          [-1.9467, -1.9809, -2.0152,  ...,  1.8722,  1.8722,  1.8722],
          [-1.9467, -1.9809, -2.0152,  ...,  1.9407,  1.9920,  2.0092],
          [-1.9467, -1.9809, -2.0152,  ...,  1.9749,  2.0605,  2.1119]],
         [[ 0.4853,  0.6604,  0.9580,  ..., -0.3725, -0.3375, -0.3200],
          [ 0.6604,  0.6954,  0.7304,  ..., -0.6527, -0.6001, -0.5826],
          [ 0.9405,  0.7479,  0.3978,  ..., -1.0903, -1.0203, -0.9678],
          ...,
          [-1.8606, -1.8957, -1.9307,  ...,  2.0434,  2.0434,  2.0434],
          [-1.8606, -1.8957, -1.9307,  ...,  2.1134,  2.1660,  2.1835],
          [-1.8606, -1.8957, -1.9307,  ...,  2.1485,  2.2360,  2.2885]],
         [[ 0.7054,  0.8797,  1.1759,  ..., -0.1487, -0.1138, -0.0964],
          [ 0.8797,  0.9145,  0.9494,  ..., -0.4275, -0.3753, -0.3578],
          [ 1.1585,  0.9668,  0.6182,  ..., -0.8633, -0.7936, -0.7413],
          ...,
          [-1.6302, -1.6650, -1.6999,  ...,  2.2566,  2.2566,  2.2566],
          [-1.6302, -1.6650, -1.6999,  ...,  2.3263,  2.3786,  2.3960],
          [-1.6302, -1.6650, -1.6999,  ...,  2.3611,  2.4483,  2.5006]]],
        [[[ 0.2624,  0.3138,  0.2967,  ..., -0.4911, -0.5424, -0.4568],
          [-0.0116,  0.0569,  0.2111,  ..., -0.4568, -0.3541, -0.4739],
          [ 0.0569,  0.0912,  0.1597,  ..., -0.3027, -0.5938, -0.9534],
          ...,
          [ 1.7009,  1.6838,  1.7009,  ...,  1.9920,  2.0605,  2.0777],
          [ 1.8037,  1.7009,  1.6667,  ...,  2.0263,  1.9407,  1.8208],
          [ 1.7180,  1.7865,  1.7694,  ...,  2.0263,  2.0263,  1.9749]],
         [[ 0.3978,  0.4503,  0.4328,  ..., -0.3725, -0.4251, -0.3375],
          [ 0.1176,  0.1877,  0.3452,  ..., -0.3375, -0.2325, -0.3550],
          [ 0.1877,  0.2227,  0.2927,  ..., -0.1800, -0.4776, -0.8452],
          ...,
          [ 1.8683,  1.8508,  1.8683,  ...,  2.1660,  2.2360,  2.2535],
          [ 1.9734,  1.8683,  1.8333,  ...,  2.2010,  2.1134,  1.9909],
          [ 1.8859,  1.9559,  1.9384,  ...,  2.2010,  2.2010,  2.1485]],
         [[ 0.6182,  0.6705,  0.6531,  ..., -0.1487, -0.2010, -0.1138],
          [ 0.3393,  0.4091,  0.5659,  ..., -0.1138, -0.0092, -0.1312],
          [ 0.4091,  0.4439,  0.5136,  ...,  0.0431, -0.2532, -0.6193],
          ...,
          [ 2.0823,  2.0648,  2.0823,  ...,  2.3786,  2.4483,  2.4657],
          [ 2.1868,  2.0823,  2.0474,  ...,  2.4134,  2.3263,  2.2043],
          [ 2.0997,  2.1694,  2.1520,  ...,  2.4134,  2.4134,  2.3611]]],
        [[[ 0.5193,  0.5536,  0.5536,  ..., -0.4911, -0.5253, -0.5253],
          [ 0.5022,  0.4337,  0.3481,  ..., -0.5082, -0.5253, -0.5082],
          [ 0.5022,  0.3994,  0.3138,  ..., -0.4397, -0.4054, -0.3541],
          ...,
          [ 0.6734,  0.6906,  0.7077,  ...,  1.7523,  1.8037,  1.8550],
          [ 0.7419,  0.7419,  0.7419,  ...,  1.8208,  1.8037,  1.8037],
          [ 0.8276,  0.7933,  0.7419,  ...,  1.8379,  1.7352,  1.6667]],
         [[ 0.6604,  0.6954,  0.6954,  ..., -0.3725, -0.4076, -0.4076],
          [ 0.6429,  0.5728,  0.4853,  ..., -0.3901, -0.4076, -0.3901],
          [ 0.6429,  0.5378,  0.4503,  ..., -0.3200, -0.2850, -0.2325],
          ...,
          [ 0.8179,  0.8354,  0.8529,  ...,  1.9209,  1.9734,  2.0259],
          [ 0.8880,  0.8880,  0.8880,  ...,  1.9909,  1.9734,  1.9734],
          [ 0.9755,  0.9405,  0.8880,  ...,  2.0084,  1.9034,  1.8333]],
         [[ 0.8797,  0.9145,  0.9145,  ..., -0.1487, -0.1835, -0.1835],
          [ 0.8622,  0.7925,  0.7054,  ..., -0.1661, -0.1835, -0.1661],
          [ 0.8622,  0.7576,  0.6705,  ..., -0.0964, -0.0615, -0.0092],
          ...,
          [ 1.0365,  1.0539,  1.0714,  ...,  2.1346,  2.1868,  2.2391],
          [ 1.1062,  1.1062,  1.1062,  ...,  2.2043,  2.1868,  2.1868],
          [ 1.1934,  1.1585,  1.1062,  ...,  2.2217,  2.1171,  2.0474]]]],
       device='cuda:0')
y: tensor([], device='cuda:0', size=(10, 0))
Using cache found in /home/aiotlab/.cache/torch/hub/pytorch_vision_v0.10.0
  0%|                                                                                             | 0/1000 [00:10<?, ?it/s]
multiprocessing.pool.RemoteTraceback:
"""
Traceback (most recent call last):
  File "/home/aiotlab/anaconda3/envs/longvaipe/lib/python3.8/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/home/aiotlab/anaconda3/envs/longvaipe/lib/python3.8/multiprocessing/pool.py", line 48, in mapstar
    return list(map(*args))
  File "/mnt/disk1/dungnt/federated-learning-dqn/utils/trainer.py", line 40, in train
    _, start_inference_loss = test(model, train_dataloader)
  File "/mnt/disk1/dungnt/federated-learning-dqn/utils/trainer.py", line 90, in test
    output = model(X)
  File "/home/aiotlab/anaconda3/envs/longvaipe/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/aiotlab/anaconda3/envs/longvaipe/lib/python3.8/site-packages/torchvision/models/resnet.py", line 249, in forward
    return self._forward_impl(x)
  File "/home/aiotlab/anaconda3/envs/longvaipe/lib/python3.8/site-packages/torchvision/models/resnet.py", line 232, in _forward_impl
    x = self.conv1(x)
  File "/home/aiotlab/anaconda3/envs/longvaipe/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/aiotlab/anaconda3/envs/longvaipe/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 446, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/aiotlab/anaconda3/envs/longvaipe/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 442, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED
"""
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File "train.py", line 344, in <module>
    main(args)
  File "train.py", line 199, in main
    pool.map(
  File "/home/aiotlab/anaconda3/envs/longvaipe/lib/python3.8/multiprocessing/pool.py", line 364, in map
    return self._map_async(func, iterable, mapstar, chunksize).get()
  File "/home/aiotlab/anaconda3/envs/longvaipe/lib/python3.8/multiprocessing/pool.py", line 771, in get
    raise self._value
RuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED